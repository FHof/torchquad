{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe46b3f0",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Import torchquad integrators and helper functions\n",
    "from torchquad import Trapezoid, Simpson, Boole, MonteCarlo, VEGAS\n",
    "from torchquad import set_precision, set_log_level, enable_cuda\n",
    "\n",
    "# Import numerical backend functions\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Imports for time measurement, plotting, etc.\n",
    "from autoray import numpy as anp\n",
    "from autoray import to_numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.benchmark as benchmark\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "# Setup needed so that Torch works with CUDA and Tensorflow does not crash\n",
    "enable_cuda()\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "use_double_precision = False\n",
    "if use_double_precision:\n",
    "    set_precision(\"double\", backend=\"torch\")\n",
    "    set_precision(\"double\", backend=\"jax\")\n",
    "else:\n",
    "    set_precision(\"float\", backend=\"torch\")\n",
    "    set_precision(\"float\", backend=\"jax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show if CUDA is enabled for the backends\n",
    "print(\n",
    "    f\"JAX: devices: {jax.devices()}, default device:\"\n",
    "    f\" {jnp.array(1.0).device_buffer.device()}\"\n",
    ")\n",
    "print(\n",
    "    f\"Tensorflow: built with CUDA: {tf.test.is_built_with_cuda()}, devices:\"\n",
    "    f\" {tf.config.list_physical_devices('GPU')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Torch: CUDA enabled: {torch.cuda.is_initialized()}, \"\n",
    "    f\"CUDA support and version: {torch.cuda.is_available()},\"\n",
    "    f\" {torch.version.cuda}, cuDNN support and version: {torch.backends.cudnn.enabled},\"\n",
    "    f\" {torch.backends.cudnn.version()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6941ccaa",
   "metadata": {},
   "source": [
    "# Definition of Example Functions and Integration Domains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc307f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_to_backends(integration_domain):\n",
    "    \"\"\"\n",
    "    Convert an integration domain given as list to tensors for each backend\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"numpy\": np.array(integration_domain),\n",
    "        \"torch\": torch.tensor(integration_domain),\n",
    "        \"jax\": jnp.array(integration_domain),\n",
    "        \"tensorflow\": tf.constant(integration_domain),\n",
    "    }\n",
    "\n",
    "\n",
    "def gaussian(a, b, c, x):\n",
    "    \"\"\"\n",
    "    Gaussian function, as defined at\n",
    "    https://en.wikipedia.org/wiki/Gaussian_function\n",
    "    a: vertical size\n",
    "    b: centre position (dim)\n",
    "    c: horizontal size\n",
    "    x: points to be evaluated (N x dim)\n",
    "    \"\"\"\n",
    "    off = x - b\n",
    "    length_sqr = anp.sum(off * off, axis=1)\n",
    "    return a * anp.exp(-length_sqr / (2.0 * c**2))\n",
    "\n",
    "\n",
    "def gaussian_peaks(x):\n",
    "    \"\"\"An example integrand function which is costly to evaluate\"\"\"\n",
    "    centres = [[0.0, 0.0], [0.5, 0.5], [0.6, 0.8], [1.0, 0.2]]\n",
    "    vertical_sizes = [2.0, 0.3, 1.0, 1.0]\n",
    "    horizontal_sizes = [0.1, 0.3, 0.05, 0.05]\n",
    "    for k in range(21):\n",
    "        cx = k / 20.0\n",
    "        cy = cx\n",
    "        centres.append([cx, cy])\n",
    "        vertical_sizes.append(1.0 * (1.0 if k % 2 == 0 else -1.0))\n",
    "        horizontal_sizes.append(0.05)\n",
    "    values = []\n",
    "    for a, b, c in zip(vertical_sizes, centres, horizontal_sizes):\n",
    "        b = anp.array(b, like=x)\n",
    "        values.append(gaussian(a, b, c, x))\n",
    "    result = sum(values)\n",
    "    return result\n",
    "\n",
    "\n",
    "functions = {\n",
    "    \"sin_2d\": {\n",
    "        \"funcs\": {\n",
    "            \"numpy\": lambda x: np.prod(np.sin(x), axis=1),\n",
    "            \"torch\": lambda x: torch.prod(torch.sin(x), dim=1),\n",
    "            \"jax\": lambda x: jnp.prod(jnp.sin(x), axis=1),\n",
    "            \"tensorflow\": lambda x: tf.reduce_prod(tf.sin(x), axis=1),\n",
    "        },\n",
    "        \"domains\": domain_to_backends([[0.0, 1.0]] * 2),\n",
    "        \"dim\": 2,\n",
    "        \"solution\": (1.0 - np.cos(1.0)) ** 2,\n",
    "    },\n",
    "    \"sin_5d\": {\n",
    "        \"funcs\": {\n",
    "            \"numpy\": lambda x: np.prod(np.sin(x), axis=1),\n",
    "            \"torch\": lambda x: torch.prod(torch.sin(x), dim=1),\n",
    "            \"jax\": lambda x: jnp.prod(jnp.sin(x), axis=1),\n",
    "            \"tensorflow\": lambda x: tf.reduce_prod(tf.sin(x), axis=1),\n",
    "        },\n",
    "        \"domains\": domain_to_backends([[0.0, 1.0]] * 5),\n",
    "        \"dim\": 5,\n",
    "        \"solution\": (1.0 - np.cos(1.0)) ** 5,\n",
    "    },\n",
    "    \"exp_2d\": {\n",
    "        \"funcs\": {\n",
    "            \"numpy\": lambda x: np.prod(np.exp(x), axis=1),\n",
    "            \"torch\": lambda x: torch.prod(torch.exp(x), dim=1),\n",
    "            \"jax\": lambda x: jnp.prod(jnp.exp(x), axis=1),\n",
    "            \"tensorflow\": lambda x: tf.reduce_prod(tf.exp(x), axis=1),\n",
    "        },\n",
    "        \"domains\": domain_to_backends([[-10.0, 10.0]] * 2),\n",
    "        \"dim\": 2,\n",
    "        \"solution\": np.exp(20.0) - 2.0 + np.exp(-20.0),\n",
    "    },\n",
    "    \"steps_extreme\": {\n",
    "        \"funcs\": {\n",
    "            \"numpy\": lambda x: np.where(\n",
    "                x[:, 0] < 0.0, 1e-17, np.where(x[:, 1] < 0.0, 1.0, -1.0)\n",
    "            ),\n",
    "            \"torch\": lambda x: torch.where(\n",
    "                x[:, 0] < 0.0,\n",
    "                torch.tensor(1e-17),\n",
    "                torch.where(x[:, 1] < 0.0, torch.tensor(1.0), torch.tensor(-1.0)),\n",
    "            ),\n",
    "            \"jax\": lambda x: jnp.where(\n",
    "                x[:, 0] < 0.0, 1e-17, jnp.where(x[:, 1] < 0.0, 1.0, -1.0)\n",
    "            ),\n",
    "            \"tensorflow\": lambda x: tf.where(\n",
    "                x[:, 0] < 0.0, 1e-17, tf.where(x[:, 1] < 0.0, 1.0, -1.0)\n",
    "            ),\n",
    "        },\n",
    "        \"domains\": domain_to_backends([[-1.0, 1.0]] * 2),\n",
    "        \"dim\": 2,\n",
    "        \"solution\": 2e-17,\n",
    "    },\n",
    "    \"gaussian_peaks_2d\": {\n",
    "        \"funcs\": {\n",
    "            \"numpy\": gaussian_peaks,\n",
    "            \"torch\": gaussian_peaks,\n",
    "            # \"jax\": jax.jit(gaussian_peaks),\n",
    "            \"jax\": gaussian_peaks,\n",
    "            \"tensorflow\": gaussian_peaks,\n",
    "        },\n",
    "        \"domains\": domain_to_backends([[0.0, 1.0]] * 2),\n",
    "        \"dim\": 2,\n",
    "        \"solution\": 0.1937366401593782,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "for backend, fun in functions[\"steps_extreme\"][\"funcs\"].items():\n",
    "    assert fun(anp.array([[-0.5, -0.5]], like=backend)) == 1e-17\n",
    "    assert fun(anp.array([[0.5, -0.5]], like=backend)) == 1.0\n",
    "    assert fun(anp.array([[0.5, 0.5]], like=backend)) == -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4d804",
   "metadata": {},
   "source": [
    "Here I defined the following functions and corresponding integration domains:\n",
    "\n",
    "* $sin\\_2d(x) = \\prod_{d=1}^{2} \\sin(x_d)$  \n",
    "  domain: $[0, 1]^2$\n",
    "* $sin\\_5d(x) = \\prod_{d=1}^{5} \\sin(x_d)$  \n",
    "  domain: $[0, 1]^5$\n",
    "* $exp\\_2d(x) = \\prod_{d=1}^{2} \\exp(x_d)$  \n",
    "  domain: $[-10, 10]^2$\n",
    "* $steps\\_extreme(x) = \\begin{cases} \\epsilon, && x_1 < 0 \\\\ 1 && x_1 \\geq 0 \\land x_2 < 0 \\\\ -1, && \\text{ otherwise }\\end{cases}$  \n",
    "  domain: $[-1, 1]^2$\n",
    "  \n",
    "The 2D functions can be visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185fe24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_integrand(func, domain, title, n_per_dim=30):\n",
    "    \"\"\"Plot a two-dimensional integrand function\"\"\"\n",
    "    X, Y = np.meshgrid(\n",
    "        np.linspace(domain[0][0], domain[0][1], n_per_dim),\n",
    "        np.linspace(domain[1][0], domain[1][1], n_per_dim),\n",
    "    )\n",
    "    points = np.stack([X, Y], axis=2).reshape([-1, 2])\n",
    "    values = func(points).reshape([n_per_dim, n_per_dim])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0, 0, 1, 1], projection=\"3d\")\n",
    "    ax.plot_surface(X, Y, values, rstride=1, cstride=1)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_plots():\n",
    "    for function_name in [\"sin_2d\", \"exp_2d\", \"steps_extreme\"]:\n",
    "        function_data = functions[function_name]\n",
    "        domain = function_data[\"domains\"][\"numpy\"]\n",
    "        func = function_data[\"funcs\"][\"numpy\"]\n",
    "        plot_2d_integrand(func, domain, function_name)\n",
    "\n",
    "\n",
    "show_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e57c87",
   "metadata": {},
   "source": [
    "# Accuracy Comparisons\n",
    "\n",
    "Since the integration rules are the same for all backends, the accuracies should only differ significantly if the backends use different floating point numbers or sum up elements in a different orders.\n",
    "\n",
    "For the Newton Cotes quadrature rules, Torch orders the points differently than JAX and Numpy because of the [different meshgrid behaviour](https://github.com/pytorch/pytorch/issues/15301).\n",
    "\n",
    "For the JAX and Torch backends, the precision can be set at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a174eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(function_name, integrator, N):\n",
    "    \"\"\"Calculate absolute and relative errors for the given function and parameters\"\"\"\n",
    "    function_data = functions[function_name]\n",
    "    errors = {}\n",
    "    for backend, func in function_data[\"funcs\"].items():\n",
    "        result = integrator.integrate(\n",
    "            func,\n",
    "            dim=function_data[\"dim\"],\n",
    "            N=N,\n",
    "            integration_domain=function_data[\"domains\"][backend],\n",
    "        )\n",
    "        result = to_numpy(result)\n",
    "        solution = function_data[\"solution\"]\n",
    "        errors[backend] = {\n",
    "            \"error_abs\": np.abs(result - solution),\n",
    "            \"error_rel\": np.abs((result - solution) / solution),\n",
    "            \"N\": N,\n",
    "        }\n",
    "    return errors\n",
    "\n",
    "\n",
    "def plot_errors(errors, title):\n",
    "    \"\"\"Show calculated errors in a bar plot\"\"\"\n",
    "    backends = sorted(errors.keys())\n",
    "    absolute_errors = [float(errors[backend][\"error_abs\"]) for backend in backends]\n",
    "    print(f\"Plotting bars {backends}, {absolute_errors}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    ax.bar(backends, absolute_errors)\n",
    "    ax.set_title(title)\n",
    "    # ~ ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"absolute error\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_accuracies():\n",
    "    errors_sin_2d = calculate_errors(\"sin_2d\", Boole(), 994009)\n",
    "    errors_sin_5d = calculate_errors(\"sin_5d\", Boole(), 13**5)\n",
    "    errors_exp = calculate_errors(\"exp_2d\", Boole(), 994009)\n",
    "    errors_steps = calculate_errors(\"steps_extreme\", Trapezoid(), 100**2)\n",
    "\n",
    "    plot_errors(errors_sin_2d, \"Boole errors for the 2D sin function\")\n",
    "    plot_errors(errors_sin_2d, \"Boole errors for the 5D sin function\")\n",
    "    plot_errors(errors_exp, \"Boole errors for the 2D exp function\")\n",
    "    plot_errors(errors_steps, \"Trapezoid errors for the function with steps\")\n",
    "\n",
    "\n",
    "compare_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006d595",
   "metadata": {},
   "source": [
    "# Runtime Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ns(dim, max_N, integrator_name, base):\n",
    "    \"\"\"\n",
    "    Calculate a list of valid number of points for the given integrator\n",
    "    which can be used to compare runtimes.\n",
    "    The number of points is roughly defined by the sequence (base ^ k)_k\n",
    "    \"\"\"\n",
    "    Ns = []\n",
    "    N_desired = 1.0\n",
    "    for _ in range(1000):\n",
    "        N = -1\n",
    "        if integrator_name == \"MonteCarlo\":\n",
    "            N = int(N_desired)\n",
    "        elif integrator_name == \"Boole\":\n",
    "            n_per_dim = int(N_desired ** (1.0 / dim) + 1e-8)\n",
    "            if n_per_dim < 5:\n",
    "                n_per_dim = 5\n",
    "            new_n_per_dim = n_per_dim - ((n_per_dim - 1) % 4)\n",
    "            N = new_n_per_dim**dim\n",
    "        elif integrator_name == \"Simpson\":\n",
    "            n_per_dim = int(N_desired ** (1.0 / dim) + 1e-8)\n",
    "            if n_per_dim < 3:\n",
    "                n_per_dim = 3\n",
    "            new_n_per_dim = n_per_dim - ((n_per_dim - 1) % 2)\n",
    "            N = new_n_per_dim**dim\n",
    "        elif integrator_name == \"Trapezoid\":\n",
    "            n_per_dim = int(N_desired ** (1.0 / dim) + 1e-8)\n",
    "            if n_per_dim < 2:\n",
    "                n_per_dim = 2\n",
    "            N = n_per_dim**dim\n",
    "        if N > max_N:\n",
    "            break\n",
    "        if (len(Ns) == 0 or Ns[-1] < N) and N >= 1:\n",
    "            Ns.append(N)\n",
    "        # Increase N roughly exponentially with the given base\n",
    "        N_desired = N_desired * base\n",
    "    return Ns\n",
    "\n",
    "\n",
    "def compare_runtimes_torchbench(function_name, integrators, max_N):\n",
    "    \"\"\"Measure times with torch's benchmark function\"\"\"\n",
    "    print(f\"Comparing runtimes for {function_name}\")\n",
    "    set_log_level(\"SUCCESS\")\n",
    "    function_data = functions[function_name]\n",
    "    measurements = []\n",
    "    measurements_info = []\n",
    "    for integrator, (backend, func) in product(\n",
    "        integrators, function_data[\"funcs\"].items()\n",
    "    ):\n",
    "        dim = function_data[\"dim\"]\n",
    "        domain = function_data[\"domains\"][backend]\n",
    "        integrator_name = type(integrator).__name__\n",
    "        for N in get_Ns(dim, max_N, integrator_name, 10):\n",
    "            print(f\"integrator: {integrator_name}, N: {N}, backend: {backend}\")\n",
    "            measurements.append(\n",
    "                benchmark.Timer(\n",
    "                    stmt=\"to_numpy(integrator.integrate(func, dim=dim, N=N, integration_domain=domain))\",\n",
    "                    globals={\n",
    "                        \"to_numpy\": to_numpy,\n",
    "                        \"integrator\": integrator,\n",
    "                        \"func\": func,\n",
    "                        \"dim\": dim,\n",
    "                        \"N\": N,\n",
    "                        \"domain\": domain,\n",
    "                    },\n",
    "                    # num_treads probably only affects torch\n",
    "                    num_threads=torch.get_num_threads(),\n",
    "                    label=f\"{function_name}, {integrator_name}\",\n",
    "                    sub_label=f\"N: {N}\",\n",
    "                    description=backend,\n",
    "                ).blocked_autorange(min_run_time=1)\n",
    "            )\n",
    "            measurements_info.append(\n",
    "                {\n",
    "                    \"integrator_name\": integrator_name,\n",
    "                    \"N\": N,\n",
    "                    \"backend\": backend,\n",
    "                    \"median\": measurements[-1].median,\n",
    "                }\n",
    "            )\n",
    "    set_log_level(\"INFO\")\n",
    "    compare = benchmark.Compare(measurements)\n",
    "    compare.trim_significant_figures()\n",
    "    compare.colorize()\n",
    "    compare.print()\n",
    "    return measurements, measurements_info\n",
    "\n",
    "\n",
    "def compare_runtimes_direct(function_name, integrators, max_N):\n",
    "    \"\"\"Measure times directly with time.perf_counter\"\"\"\n",
    "    print(f\"Collecting runtimes for {function_name}\")\n",
    "    set_log_level(\"SUCCESS\")\n",
    "    function_data = functions[function_name]\n",
    "    measurements_info = []\n",
    "    for integrator, (backend, func) in product(\n",
    "        integrators, function_data[\"funcs\"].items()\n",
    "    ):\n",
    "        dim = function_data[\"dim\"]\n",
    "        domain = function_data[\"domains\"][backend]\n",
    "\n",
    "        def run_integration(N):\n",
    "            return to_numpy(\n",
    "                integrator.integrate(func, dim=dim, N=N, integration_domain=domain)\n",
    "            )\n",
    "\n",
    "        integrator_name = type(integrator).__name__\n",
    "        for N in get_Ns(dim, max_N, integrator_name, 10):\n",
    "            print(f\"integrator: {integrator_name}, N: {N}, backend: {backend}\")\n",
    "            t0 = time.perf_counter()\n",
    "            run_integration(N)\n",
    "            t1 = time.perf_counter()\n",
    "            run_integration(N)\n",
    "            runtime2 = time.perf_counter() - t1\n",
    "            runtime1 = t1 - t0\n",
    "            measurements_info.append(\n",
    "                {\n",
    "                    \"integrator_name\": integrator_name,\n",
    "                    \"N\": N,\n",
    "                    \"backend\": backend,\n",
    "                    \"runtime1\": runtime1,\n",
    "                    \"runtime2\": runtime2,\n",
    "                }\n",
    "            )\n",
    "    set_log_level(\"INFO\")\n",
    "    return measurements_info\n",
    "\n",
    "\n",
    "measured_function_name = \"gaussian_peaks_2d\"\n",
    "# measurements, measurements_info = compare_runtimes(\n",
    "#    measured_function_name, [Trapezoid(), Boole(), MonteCarlo()], max_N=1000000\n",
    "# )\n",
    "measurements_info = compare_runtimes_direct(\n",
    "    measured_function_name, [Trapezoid(), Boole(), MonteCarlo()], max_N=1000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(\n",
    "    xs,\n",
    "    ys,\n",
    "    labels,\n",
    "    title,\n",
    "    ylabel,\n",
    "    xlabel,\n",
    "    xscale_log=False,\n",
    "    yscale_log=False,\n",
    "    output_file=None,\n",
    "):\n",
    "    \"\"\"Create line plots\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(title)\n",
    "\n",
    "    for x, y, label in zip(xs, ys, labels):\n",
    "        ax.plot(x, y, \".-\", label=label)\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    if xscale_log:\n",
    "        ax.set_xscale(\"log\")\n",
    "    if yscale_log:\n",
    "        ax.set_yscale(\"log\")\n",
    "    else:\n",
    "        ax.set_ylim(bottom=0)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.0, 0.5))\n",
    "\n",
    "    if output_file is not None:\n",
    "        fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_runtimes_torchbench(measurements_info, integrator_name):\n",
    "    \"\"\"Plot measurements collected with compare_runtimes_torchbench\"\"\"\n",
    "    Ns = {}\n",
    "    medians = {}\n",
    "    for info in measurements_info:\n",
    "        if info[\"integrator_name\"] != integrator_name:\n",
    "            continue\n",
    "        backend = info[\"backend\"]\n",
    "        if backend not in Ns:\n",
    "            Ns[backend] = []\n",
    "            medians[backend] = []\n",
    "        Ns[backend].append(info[\"N\"])\n",
    "        medians[backend].append(info[\"median\"])\n",
    "    xs = []\n",
    "    ys = []\n",
    "    labels = []\n",
    "    for backend, Nvalues in Ns.items():\n",
    "        xs.append(Nvalues)\n",
    "        ys.append(medians[backend])\n",
    "        labels.append(backend)\n",
    "    plot_lines(\n",
    "        xs,\n",
    "        ys,\n",
    "        labels,\n",
    "        title=f\"Runtimes for integrator {integrator_name}, integrand {measured_function_name}\",\n",
    "        xlabel=\"Number of evaluations\",\n",
    "        ylabel=\"Median time in s\",\n",
    "        xscale_log=True,\n",
    "        yscale_log=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_runtimes_direct(measurements_info, integrator_name):\n",
    "    \"\"\"Plot measurements collected with compare_runtimes_direct\"\"\"\n",
    "    Ns = {}\n",
    "    runtimes = {}\n",
    "    for info in measurements_info:\n",
    "        if info[\"integrator_name\"] != integrator_name:\n",
    "            continue\n",
    "        backend = info[\"backend\"]\n",
    "        if backend not in Ns:\n",
    "            Ns[backend] = []\n",
    "            runtimes[backend] = []\n",
    "        Ns[backend].append(info[\"N\"])\n",
    "        runtimes[backend].append([info[\"runtime1\"], info[\"runtime2\"]])\n",
    "    xs = []\n",
    "    ys = []\n",
    "    labels = []\n",
    "    for backend, Nvalues in Ns.items():\n",
    "        xs.append(Nvalues)\n",
    "        ys.append([rt[0] for rt in runtimes[backend]])\n",
    "        labels.append(f\"{backend}, first run\")\n",
    "        xs.append(Nvalues)\n",
    "        ys.append([rt[1] for rt in runtimes[backend]])\n",
    "        labels.append(f\"{backend}, second run\")\n",
    "    plot_lines(\n",
    "        xs,\n",
    "        ys,\n",
    "        labels,\n",
    "        title=f\"Runtimes for integrator {integrator_name}, integrand {measured_function_name}\",\n",
    "        xlabel=\"Number of evaluations\",\n",
    "        ylabel=\"perf_counter time in s\",\n",
    "        xscale_log=True,\n",
    "        yscale_log=True,\n",
    "        output_file=f\"tmp_{integrator_name}.pdf\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_all_runtimes():\n",
    "    for integrator_name in [\"MonteCarlo\", \"Trapezoid\", \"Boole\"]:\n",
    "        plot_runtimes_direct(measurements_info, integrator_name)\n",
    "\n",
    "\n",
    "plot_all_runtimes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d64801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
